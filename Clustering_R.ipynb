{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ3gVutJlmwd",
        "outputId": "67e7ee9a-4c28-438c-d9aa-cfded8f2381e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m48284\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m9\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (2): tweet_text, processed_tweet\n",
            "\u001b[32mdbl\u001b[39m (7): age, ethnicity, gender, religion, other_cyberbullying, not_cyberbul...\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m# A tibble: 48,284 × 9\u001b[39m\n",
            "   tweet_text                  age ethnicity gender religion other_cyberbullying\n",
            "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m                     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m  \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m               \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
            "\u001b[90m 1\u001b[39m In other words #katandan…     0         0      0        0                   0\n",
            "\u001b[90m 2\u001b[39m Why is #aussietv so whit…     0         0      0        0                   0\n",
            "\u001b[90m 3\u001b[39m @XochitlSuckkks a classy…     0         0      0        0                   0\n",
            "\u001b[90m 4\u001b[39m @Jason_Gio meh. :P  than…     0         0      0        0                   0\n",
            "\u001b[90m 5\u001b[39m @RudhoeEnglish This is a…     0         0      0        0                   0\n",
            "\u001b[90m 6\u001b[39m @Raja5aab @Quickieleaks …     0         0      0        0                   0\n",
            "\u001b[90m 7\u001b[39m Itu sekolah ya bukan tem…     0         0      0        0                   0\n",
            "\u001b[90m 8\u001b[39m Karma. I hope it bites K…     0         0      0        0                   0\n",
            "\u001b[90m 9\u001b[39m @stockputout everything …     0         0      0        0                   0\n",
            "\u001b[90m10\u001b[39m Rebecca Black Drops Out …     0         0      0        0                   0\n",
            "\u001b[90m# ℹ 48,274 more rows\u001b[39m\n",
            "\u001b[90m# ℹ 3 more variables: not_cyberbullying <dbl>, sexism <dbl>,\u001b[39m\n",
            "\u001b[90m#   processed_tweet <chr>\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "# Load necessary library\n",
        "library(readr)\n",
        "\n",
        "# Define the file path\n",
        "file_path <- '/content/Final.csv'\n",
        "\n",
        "# Read the CSV file into a dataframe\n",
        "dataset <- read_csv(file_path)\n",
        "\n",
        "# Display the dataset\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jacRChLSlz7p",
        "outputId": "3818ac1d-2d5c-4536-dfd1-83443b384952"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"text2vec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agZ_dizbmCX5",
        "outputId": "780280ab-f477-42b5-d30d-6c974eb3876b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "library(text2vec)\n",
        "library(dplyr)\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and 'processed_tweet' contains preprocessed tweet texts\n",
        "\n",
        "# If 'processed_tweet' contains strings of list-like text, first remove unwanted characters\n",
        "processed_tweet <- gsub(\"[\\\\[\\\\]',]\", \"\", dataset$processed_tweet)\n",
        "\n",
        "# Tokenize the processed tweets\n",
        "it <- itoken(processed_tweet,\n",
        "             preprocessor = tolower,\n",
        "             tokenizer = word_tokenizer,\n",
        "             progressbar = FALSE)\n",
        "\n",
        "# Create a vocabulary and prune it to have a maximum of 1000 terms\n",
        "vocab <- create_vocabulary(it) %>%\n",
        "  prune_vocabulary(term_count_min = 1,\n",
        "                   doc_proportion_max = 0.5,\n",
        "                   vocab_term_max = 1000)\n",
        "\n",
        "# Vectorize the text\n",
        "vectorizer <- vocab_vectorizer(vocab)\n",
        "dtm <- create_dtm(it, vectorizer)\n",
        "\n",
        "# Convert the Document-Term Matrix (DTM) to a DataFrame\n",
        "tfidf <- TfIdf$new()\n",
        "tfidf_matrix <- fit_transform(dtm, tfidf)\n",
        "tfidf_df <- as.data.frame(as.matrix(tfidf_matrix))\n",
        "\n",
        "# Display the first few rows of the TF-IDF DataFrame\n",
        "head(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "oj2HXbF7mQyZ",
        "outputId": "55503f82-8530-400d-8dec-950b1a71d02b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 1000</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>bully</th><th scope=col>school</th><th scope=col>fuck</th><th scope=col>like</th><th scope=col>girl</th><th scope=col>nigger</th><th scope=col>joke</th><th scope=col>dumb</th><th scope=col>high</th><th scope=col>people</th><th scope=col>⋯</th><th scope=col>minority</th><th scope=col>plus</th><th scope=col>racial</th><th scope=col>writing</th><th scope=col>afraid</th><th scope=col>ah</th><th scope=col>alive</th><th scope=col>decided</th><th scope=col>ex</th><th scope=col>fall</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>0</td><td>0</td><td>0</td><td>0.386187</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 1000\n\n| <!--/--> | bully &lt;dbl&gt; | school &lt;dbl&gt; | fuck &lt;dbl&gt; | like &lt;dbl&gt; | girl &lt;dbl&gt; | nigger &lt;dbl&gt; | joke &lt;dbl&gt; | dumb &lt;dbl&gt; | high &lt;dbl&gt; | people &lt;dbl&gt; | ⋯ ⋯ | minority &lt;dbl&gt; | plus &lt;dbl&gt; | racial &lt;dbl&gt; | writing &lt;dbl&gt; | afraid &lt;dbl&gt; | ah &lt;dbl&gt; | alive &lt;dbl&gt; | decided &lt;dbl&gt; | ex &lt;dbl&gt; | fall &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 | 0 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 2 | 0 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 3 | 0 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | 0 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | 0 | 0 | 0 | 0.386187 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n| 6 | 0 | 0 | 0 | 0.000000 | 0 | 0 | 0 | 0 | 0 | 0 | ⋯ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n\n",
            "text/latex": "A data.frame: 6 × 1000\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & bully & school & fuck & like & girl & nigger & joke & dumb & high & people & ⋯ & minority & plus & racial & writing & afraid & ah & alive & decided & ex & fall\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t1 & 0 & 0 & 0 & 0.000000 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\t2 & 0 & 0 & 0 & 0.000000 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\t3 & 0 & 0 & 0 & 0.000000 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\t4 & 0 & 0 & 0 & 0.000000 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\t5 & 0 & 0 & 0 & 0.386187 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\t6 & 0 & 0 & 0 & 0.000000 & 0 & 0 & 0 & 0 & 0 & 0 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  bully school fuck like     girl nigger joke dumb high people ⋯ minority plus\n",
              "1 0     0      0    0.000000 0    0      0    0    0    0      ⋯ 0        0   \n",
              "2 0     0      0    0.000000 0    0      0    0    0    0      ⋯ 0        0   \n",
              "3 0     0      0    0.000000 0    0      0    0    0    0      ⋯ 0        0   \n",
              "4 0     0      0    0.000000 0    0      0    0    0    0      ⋯ 0        0   \n",
              "5 0     0      0    0.386187 0    0      0    0    0    0      ⋯ 0        0   \n",
              "6 0     0      0    0.000000 0    0      0    0    0    0      ⋯ 0        0   \n",
              "  racial writing afraid ah alive decided ex fall\n",
              "1 0      0       0      0  0     0       0  0   \n",
              "2 0      0       0      0  0     0       0  0   \n",
              "3 0      0       0      0  0     0       0  0   \n",
              "4 0      0       0      0  0     0       0  0   \n",
              "5 0      0       0      0  0     0       0  0   \n",
              "6 0      0       0      0  0     0       0  0   "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "library(stats)\n",
        "library(ggplot2)\n",
        "\n",
        "# Assuming 'tfidf_df' is your DataFrame containing the TF-IDF features\n",
        "\n",
        "# Determine the optimal number of clusters using the elbow method\n",
        "inertia <- numeric()\n",
        "K <- 1:10\n",
        "set.seed(42)  # For reproducibility\n",
        "\n",
        "for (k in K) {\n",
        "  kmeans_model <- kmeans(tfidf_df, centers = k, nstart = 5)\n",
        "  inertia <- c(inertia, kmeans_model$tot.withinss)\n",
        "}\n",
        "\n",
        "# Plot the elbow curve\n",
        "elbow_plot <- data.frame(K = K, Inertia = inertia)\n",
        "options(repr.plot.width=10, repr.plot.height=6)\n",
        "ggplot(elbow_plot, aes(x = K, y = Inertia)) +\n",
        "  geom_point() +\n",
        "  geom_line() +\n",
        "  xlab('Number of clusters') +\n",
        "  ylab('Inertia') +\n",
        "  ggtitle('Elbow Method For Optimal k') +\n",
        "  theme_minimal()\n"
      ],
      "metadata": {
        "id": "pYYqjtKUnvoP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "library(stats)\n",
        "library(dplyr)\n",
        "\n",
        "# Assuming 'tfidf_df' is your DataFrame containing the TF-IDF features\n",
        "\n",
        "# Perform K-Means clustering with the optimal number of clusters (e.g., 4 clusters)\n",
        "set.seed(42)  # For reproducibility\n",
        "kmeans_result <- kmeans(tfidf_df, centers = 4, nstart = 5)\n",
        "\n",
        "# Add the cluster labels to the original dataset\n",
        "dataset$cluster <- kmeans_result$cluster\n",
        "\n",
        "# Show the first few rows of the dataset with the cluster labels\n",
        "head(dataset)\n"
      ],
      "metadata": {
        "id": "whZU-ywEn2_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "library(ggplot2)\n",
        "library(stats)\n",
        "\n",
        "# Assuming 'tfidf_df' is your DataFrame containing the TF-IDF features\n",
        "# and 'dataset' has the cluster labels\n",
        "\n",
        "# Reduce the dimensions to 2D using PCA\n",
        "pca_result <- prcomp(tfidf_df, center = TRUE, scale. = TRUE)\n",
        "X_pca <- data.frame(pca_result$x[, 1:2])\n",
        "\n",
        "# Add the cluster labels to the PCA result for plotting\n",
        "X_pca$cluster <- as.factor(dataset$cluster)\n",
        "\n",
        "# Plot the clusters\n",
        "options(repr.plot.width=12, repr.plot.height=8)\n",
        "ggplot(X_pca, aes(x = PC1, y = PC2, color = cluster)) +\n",
        "  geom_point(size = 2) +\n",
        "  labs(title = \"Clusters Visualization\", x = \"PCA Component 1\", y = \"PCA Component 2\") +\n",
        "  theme_minimal() +\n",
        "  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\"))\n"
      ],
      "metadata": {
        "id": "IIz46ILRtjC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'dataset' is your DataFrame and it contains the 'tweet_text' and 'cluster' columns\n",
        "\n",
        "# Function to print sample tweets from each cluster\n",
        "print_cluster_samples <- function(cluster_number) {\n",
        "  cat(sprintf(\"\\nCluster %d Sample Tweets:\\n\", cluster_number))\n",
        "  cluster_samples <- dataset %>%\n",
        "    filter(cluster == cluster_number) %>%\n",
        "    select(tweet_text) %>%\n",
        "    sample_n(5, replace = FALSE, set.seed(42))\n",
        "\n",
        "  for (tweet in cluster_samples$tweet_text) {\n",
        "    cat(sprintf(\"- %s\\n\", tweet))\n",
        "  }\n",
        "}\n",
        "\n",
        "# Print sample tweets from each cluster\n",
        "for (cluster_num in 1:4) {\n",
        "  print_cluster_samples(cluster_num)\n",
        "}\n"
      ],
      "metadata": {
        "id": "Lel3Z-FOuPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"wordcloud\")"
      ],
      "metadata": {
        "id": "UIKiqDpnvKj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not already installed\n",
        "# install.packages(\"tm\")\n",
        "# install.packages(\"wordcloud\")\n",
        "# install.packages(\"ggplot2\")\n",
        "\n",
        "library(dplyr)\n",
        "library(tm)\n",
        "library(wordcloud)\n",
        "library(ggplot2)\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame with 'tweet_text' and 'cluster' columns\n",
        "\n",
        "generate_wordcloud_for_cluster <- function(cluster_number) {\n",
        "  # Filter the dataset for the given cluster number\n",
        "  cluster_data <- dataset %>%\n",
        "    filter(cluster == cluster_number)\n",
        "\n",
        "  # Create a corpus from the tweets in the cluster\n",
        "  corpus <- Corpus(VectorSource(cluster_data$tweet_text))\n",
        "\n",
        "  # Clean the text data\n",
        "  corpus <- corpus %>%\n",
        "    tm_map(content_transformer(tolower)) %>%\n",
        "    tm_map(removePunctuation) %>%\n",
        "    tm_map(removeNumbers) %>%\n",
        "    tm_map(removeWords, stopwords(\"en\")) %>%\n",
        "    tm_map(stripWhitespace)\n",
        "\n",
        "  # Create a term-document matrix\n",
        "  tdm <- TermDocumentMatrix(corpus)\n",
        "\n",
        "  # Convert the term-document matrix into a matrix\n",
        "  tdm_matrix <- as.matrix(tdm)\n",
        "\n",
        "  # Get the word frequencies\n",
        "  word_freqs <- sort(rowSums(tdm_matrix), decreasing = TRUE)\n",
        "\n",
        "  # Convert to a data frame\n",
        "  word_freqs_df <- data.frame(word = names(word_freqs), freq = word_freqs)\n",
        "\n",
        "  # Generate the word cloud\n",
        "  wordcloud(words = word_freqs_df$word, freq = word_freqs_df$freq,\n",
        "            min.freq = 2, max.words = 100, random.order = FALSE,\n",
        "            colors = brewer.pal(8, \"Dark2\"))\n",
        "}\n",
        "\n",
        "# Generate word clouds for clusters 1 to 4\n",
        "for (cluster_num in 1:4) {\n",
        "  cat(sprintf(\"\\nGenerating word cloud for Cluster %d\\n\", cluster_num))\n",
        "  generate_wordcloud_for_cluster(cluster_num)\n",
        "}\n"
      ],
      "metadata": {
        "id": "vU0ifbFaucin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2J2F2Z-HvD1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}